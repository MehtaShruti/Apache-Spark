{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%fs ls /FileStore/tables/\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%fs head /FileStore/tables/1.json"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"dbfs:/FileStore/tables/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\njsonSchema = StructType([ StructField(\"filename\", StringType(), True), StructField(\"instrument_source_str\", StringType(), True), StructField(\"instrument\", StringType(), True), StructField(\"S1\", StringType(), True), StructField(\"S2\",\nStringType(), True), StructField(\"S3\", StringType(), True), StructField(\"S4\", StringType(), True), StructField(\"S5\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstaticInputDF = (\n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n)\n\ndisplay(staticInputDF)\nstaticInputDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql select instrument_source_str, sum(S1),sum(S2),sum(S3), sum(S4), sum(S5), count(instrument_source_str) as count from static_counts group by instrument_source_str\n "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 10)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.instrument_source_str\n      )\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select instrument_source_str, count from counts order by instrument_source_str desc\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql select * from counts"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"stream","notebookId":175750683798069},"nbformat":4,"nbformat_minor":0}
